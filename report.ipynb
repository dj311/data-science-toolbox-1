{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Toolbox: Literature Review\n",
    "By Shanglin Zou and Daniel Jones\n",
    "\n",
    "We chose to divide the work equally, resulting in a corresponding split of equity:\n",
    "  - Shanglin Zou: 50%\n",
    "  - Daniel Jones: 50%\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "As part of a literature review, our key aim was to survey both the current and historical landscape of data science and it's relation to cyber-security. This covers a broad set of topics, which we broke down into the following three categories [1]:\n",
    "  1. Classical Statistics\n",
    "  2. Machine Learning\n",
    "  3. Neural Networks\n",
    "  \n",
    "We will aim to implement approaches from a couple of these fields in the practical part of this report.\n",
    " \n",
    "TODO: talk about literature\n",
    "\n",
    "Survey papers:\n",
    "  - https://ieeexplore.ieee.org/abstract/document/7307098/\n",
    "  - https://www.sciencedirect.com/science/article/pii/S108480451200183X\n",
    "\n",
    "### What are the broad types of data?\n",
    "\n",
    "- Network Analysis\n",
    "  - Common in this area\n",
    "  - secrepo.com provides a number of resources. e.g. http://www.secrepo.com/Datasets%20Description/Network/http.html and http://www.secrepo.com/Datasets%20Description/Network/weird.html\n",
    "     - these use the [bro](https://www.bro.org/) log file format describe here: http://gauss.ececs.uc.edu/Courses/c6055/pdf/bro_log_vars.pdf\n",
    "     - bro citation:\n",
    "     > \n",
    "    Vern Paxson\n",
    "    Bro: A System for Detecting Network Intruders in Real-Time\n",
    "    Computer Networks, 31(23â€”24), pp. 2435-2463, 1999.\n",
    "    ([BibTeX](https://www.bro.org/research/bro99.bib)) ([PDF](http://www.icir.org/vern/papers/bro-CN99.pdf))\n",
    "- Log Files\n",
    "    \n",
    "- Program Execution Traces:\n",
    "   - Stochastic Identification of Malware with Dynamic Traces (https://arxiv.org/pdf/1404.2462.pdf): Using program runtime traces as a data source and applying Markov chains to classify them as malware. These files are generated and can be read by https://github.com/moyix/panda-malrec (B. Dolan-Gavitt, J. Hodosh, P. Hulin, T. Leek, R. Whelan. Repeatable Reverse Engineering with PANDA. 5th Program Protection and Reverse Engineering Workshop, Los Angeles, California, December 2015.)\n",
    "\n",
    "\n",
    "\n",
    "### What are the main types of resource?\n",
    "We looked at:  books, papers, online tutorials, public code and data repositories.\n",
    "\n",
    "Books:\n",
    " - \"Introduction to Machine Learning\"\n",
    " \n",
    "Papers:\n",
    " \n",
    " - \"Visual Analytics for Model Selection in Time Series Analysis\"\n",
    " \n",
    "Take-aways:\n",
    "  - There are lots of resources on statistics, machine learning and neural networks (within books, papers and online resources) but not many that apply this to cyber-security.\n",
    "  - secrepo.com proved a valuable resource for both data sets, and analysis of those data sources.\n",
    "  - academic papers provided a combination of the two (stats applied to cyber-security data) but code samples were hard to come by\n",
    "  \n",
    "  \n",
    "### What type of problems can the resources solve?\n",
    "- The paper,\"Visual Analytics for Model Selection in Time Series Analysis\", helps us solve Time Series modelling, and we also need to find some code samples to help our project.\n",
    "\n",
    "\n",
    "### Are there any generic data science resources that might be applicable? In what sense are they applicable?\n",
    "Since there is little introductory material on data science applied to cyber-security, the generic data science resources proved useful to learn the theory behind it's application. Approach: read papers with direct application to cyber-security, and use generic data science resources to fill in knowledge gaps.\n",
    "\n",
    "\"Introduction to Machine learning\" proved to be a useful reference text to learn about k-Nearest-Neighbors classification, since the papers applying it to cyber-security assumed knowledge of the algorithm as a pre-requisite.\n",
    "\n",
    "\"Visual Analytics for Model Selection in Time Series Analysis\" is a good paper to learn model selection in Time series Analysis. The paper shows the steps we need to do when we try to model a time series data. \n",
    "\n",
    "\n",
    "### How might the approach be compared to other approaches, and/or applied across different datasets?\n",
    "For time series approach, we can easily apply it across different datasets. Resampling, testing stationary, differencing, model selection, fitting and prediction are the steps we need to do. However, i think it does not work for analyzing string data(i.e Ip address, Id)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series analysis of HTTP Bro Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Series is a good techqiue that we can use for Cybersecurity data since all the actions in cyber is related to the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "http_df = pd.read_csv(\"resources/http.log.zip\", header=None, sep=\"\\t\", \n",
    "                      names=['ts', 'uid', 'id.orig_h', 'id.orig_p', 'id.resp_h', 'id.resp_p', 'trans_depth', 'method', 'host', 'uri', 'referrer', \n",
    "                             'user_agent', 'request_body_len', 'response_body_len', 'status_code', 'status_msg', 'info_code', 'info_msg', 'filename', 'tags', \n",
    "                             'username', 'password', 'proxied', 'orig_fuids', 'orig_mime_types', 'resp_fuids', 'resp_mime_types', 'sample']) \n",
    "#resources from http://www.secrepo.com/Datasets%20Description/Network/http.html\n",
    "http_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#clean up the timestamp column ts\n",
    "from datetime import datetime\n",
    "http_df['ts'] = [datetime.fromtimestamp(float(date)) for date in http_df['ts'].values]\n",
    "http_df = http_df.set_index('ts')\n",
    "http_df.head()\n",
    "# code provided by https://github.com/sooshie/Security-Data-Analysis/blob/master/Lab_2/Lab_2-Solutions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphing time series data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We focus on two cols, which are request_body_len and response_body_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "df = http_df[['request_body_len','response_body_len']]\n",
    "ts_figure, ts_axes =plt.subplots(figsize=(16, 5))\n",
    "df.plot(ax=ts_axes)\n",
    "df.head\n",
    "np.unique(df['request_body_len'])\n",
    "# request_body_len means Actual uncompressed content size of the data transferred from the client\n",
    "# response_body_len means Actual uncompressed content size of the data transferred from the server  \n",
    "# code provided by https://github.com/sooshie/Security-Data-Analysis/blob/master/Lab_2/Lab_2-Solutions.ipynb\n",
    "# If the figure doesn't show up, just rerun this single cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows the actual umcompressed content size of data from the client and server at the end of month. You can see that blue line is very tiny which means comparing to response, it has very small amount of data size transferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resamping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "resamp = df.resample(\"M\", how=['mean', 'count', 'sum']) #\"M\" means month end frequency\n",
    "resamp.plot(subplots=True)\n",
    "# code provided by https://github.com/sooshie/Security-Data-Analysis/blob/master/Lab_2/Lab_2-Solutions.ipynb\n",
    "# If the figure doesn't show up, just rerun this single cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows that the data that resampled by mean, number of count, total data size at the end of month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resamp.plot()\n",
    "# If the figure doesn't show up, just rerun this single cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Times Series Model-- ARIMA(p,d,q)\n",
    "https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average\n",
    "\n",
    "using ACF and PACF to determine p and q.\n",
    "https://onlinecourses.science.psu.edu/stat510/node/62/\n",
    "\n",
    "ACF and PACF\n",
    "https://machinelearningmastery.com/gentle-introduction-autocorrelation-partial-autocorrelation/\n",
    "\n",
    "Code for ACF and PACF\n",
    "https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/\n",
    "\n",
    "Python code for time series\n",
    "https://machinelearningmastery.com/time-series-forecasting-methods-in-python-cheat-sheet/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try modelling the count of response_body_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARIMA model is combined by two models which are Autoregressive Model(AR) and Moving Average Model(MA)\n",
    "To determine the order of the model, i.e ARMA(p,d,q), p and q are the orders of the model, we need to plot the autocorrelation function plot(acf) to determine q, and partial autocorrelation function(pacf) plot to get p's value. d is differencing, in statistics is a transformation applied to time-series data in order to make it stationary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the big value of data, we need to take logarithm to our data and difference the data to make it stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resamp['response_count_log']=resamp['response_body_len']['count'].apply(np.log) \n",
    "resamp['response_count_log_diff'] = resamp['response_count_log'] - resamp['response_count_log'].shift()\n",
    "resamp1 = resamp['response_count_log_diff'][1:]\n",
    "resamp1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "lag_acf = acf(resamp1, nlags=20)\n",
    "#Plot ACF: \n",
    "plt.subplot(121) \n",
    "plt.plot(lag_acf)\n",
    "plt.axhline(y=0,linestyle='--',color='gray')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(resamp1)),linestyle='--',color='gray')\n",
    "plt.axhline(y=1.96/np.sqrt(len(resamp1)),linestyle='--',color='gray')\n",
    "plt.title('Autocorrelation Function')\n",
    "\n",
    "#Plot PACF:\n",
    "lag_pacf = pacf(resamp1, nlags=34, method='ols')\n",
    "plt.subplot(122)\n",
    "plt.plot(lag_pacf)\n",
    "plt.axhline(y=0,linestyle='--',color='gray')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(resamp1)),linestyle='--',color='gray')\n",
    "plt.axhline(y=1.96/np.sqrt(len(resamp1)),linestyle='--',color='gray')\n",
    "plt.title('Partial Autocorrelation Function')\n",
    "plt.tight_layout()\n",
    "# https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/ acf and pacf codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the acf plot, we can get q's value. Q's value depends on how many lags that are beyond the boundaries in the first time. [1]\n",
    "\n",
    "According to this plot, we can get that lag 1 is above the boundaries, so we can say that q is equal to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can use pacf to determine the p value by the same way. By the plot, we can get p is equal to 1.\n",
    "\n",
    "Also, we difference the data once, we can say d = 1.\n",
    "\n",
    "Therefore we could say that it is a ARIMA model with p=1, q=1, d=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model and predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Train and Test data\n",
    "\n",
    "we have 37 months of data, we set first 34 months as training data, and last 3 months as test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train data\n",
    "train = resamp1[0:33]\n",
    "test = resamp1[33:36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ARIMA\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "# fit model p=1, q=1,d=1\n",
    "model = ARIMA(train, order=(1,1,1))\n",
    "model_fit = model.fit(disp=False)\n",
    "# test the data\n",
    "yhat = model_fit.predict(len(train),(len(resamp1)))\n",
    "print(yhat) #this is the predicted value of differencing log data, and need to take it back to orignal scale.\n",
    "#convert to log scale first\n",
    "new = train.append(yhat)\n",
    "new_cumsum = new.cumsum()\n",
    "new_cum = pd.Series(resamp['response_count_log'][0], index= resamp['response_count_log'].index)\n",
    "new_cum = new_cum.add(new_cumsum,fill_value=0)\n",
    "new_cum[37]= new_cum[37] + resamp['response_count_log'][0]\n",
    "#convert to orignal scale\n",
    "np.exp(new_cum)[-1:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what we predict for August 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.exp(new_cum)[-4:-1]\n",
    "#shows the fitted data from May 2014 to July 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resamp['response_body_len']['count'][-3:]\n",
    "#actual data from May 2014 to July 2014."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is fitted data and test data. they actually have the big difference here. It may cause some problems. Maybe the model is not that good? \n",
    "Also, I think I skipped one step is checking the stationary of data, this is also what i need to notice next time. \n",
    "Besides this, this is basic way how we model a data using time series method.\n",
    "Using programing to analyze data is a good combination of classical statistics and modern machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA(1,1,1)\n",
    "![111.png](attachment:111.png)\n",
    "![222.png](attachment:222.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static Analysis and Classification of Zeus Malware\n",
    "\n",
    "The paper \"Unveiling Zeus: automated classification of malware samples\" [0] surveys a number of classification methods to differentiate binaries in the [Zeus malware family](https://en.wikipedia.org/wiki/Zeus_%28malware%29) from other types of malware. The paper used the `auto-mal` tool to perform [dynamic analysis](https://en.wikipedia.org/wiki/Dynamic_program_analysis) of the binaries as they run, creating a set of sixty-five features for each binary sample. A number of classification methods were applied and their accuracy compared.\n",
    "\n",
    "Here, the methodology above is applied to a data set generated from [static analysis](https://en.wikipedia.org/wiki/Static_program_analysis) of malware binaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "import seaborn\n",
    "import matplotlib\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import neighbors\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_state = numpy.random.RandomState(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import\n",
    "\n",
    "Source the data set from static analysis of three different families of malware:\n",
    "  1. [Zeus](https://en.wikipedia.org/wiki/Zeus_%28malware%29)\n",
    "  1. [Operation Cleaver](https://en.wikipedia.org/wiki/Operation_Cleaver)\n",
    "  1. [APT-1](https://www.fireeye.com/content/dam/fireeye-www/services/pdfs/mandiant-apt1-report.pdf)\n",
    "  \n",
    "The following uses pre-analysed files, sourced from Mike Sconzo at [SecRepo](https://secrepo.com) under a [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "zeus = pandas.read_csv(\"resources/Zeus.csv\")  # sourced from http://www.secrepo.com/Datasets%20Description/PE_malware/Zeus.html\n",
    "zeus['Source'] = 'zeus'\n",
    "zeus.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zeus.describe(include=numpy.object).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "op_cleaver = pandas.read_csv(\"resources/OPCleaver.csv\")  # sourced from http://www.secrepo.com/Datasets%20Description/PE_malware/OPCleaver.html\n",
    "op_cleaver['Source'] = 'op_cleaver'\n",
    "op_cleaver.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "op_cleaver.describe(include=numpy.object).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "virus_share = pandas.read_csv(\"resources/VirusShare.csv\")  # sourced from http://www.secrepo.com/Datasets%20Description/PE_malware/VirusShare.html\n",
    "virus_share['Source'] = 'virus_share'\n",
    "virus_share.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "virus_share.describe(include=numpy.object).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above data sets show 11 numerical features and 2 categorical (with the `Source` column added at import time). There is little documentation on how these features have been generated. They are a combination of PE file headers (e.g.`SectionAlignment` ), and further analysis (e.g. `HighEntropy`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Analysis\n",
    "\n",
    "TODO: plot a few relations between features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleanup\n",
    "\n",
    "The Zeus and Operation Cleaver data sets both have the column \"SizeOfHeaders.1\", which is missing from the APT-1/VirusShare data set. Check that these are duplicates of the \"SizeOfHeaders\" column, then delete them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if all(zeus['SizeOfHeaders'] == zeus['SizeOfHeaders.1']):\n",
    "    del zeus['SizeOfHeaders.1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if all(op_cleaver['SizeOfHeaders'] == op_cleaver['SizeOfHeaders.1']):\n",
    "    del op_cleaver['SizeOfHeaders.1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a large size disparity between the three data sets. Combine the Operation Cleaver and Virus Share data sets to create a single data set with 392 non-Zeus samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "non_zeus = op_cleaver.append(\n",
    "    virus_share, \n",
    "    ignore_index=True,  # generate new indexes for the virus_share set\n",
    ")\n",
    "non_zeus['Source'] = 'non-zeus'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of the data sets\n",
    "\n",
    "To mirror the work in the paper we will use an equal number of Zeus and non-Zeus samples in the learning data. Since the Zeus data set is much larger than that of the non-Zeus data set, take a random sample of the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zeus = zeus.sample(n=len(non_zeus), random_state=random_state)\n",
    "len(zeus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to split the data into training and testing sets. Inline with the paper, keep 10% of the data for testing. When doing this, we sample separately from the Zeus and non-Zeus data, this ensures the same number of Zeus and non-Zeus data points will be used during training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zeus_training = zeus.sample(frac=0.9, random_state=random_state)\n",
    "zeus_testing = zeus.drop(index=zeus_training.index)\n",
    "\n",
    "len(zeus_training), len(zeus_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "non_zeus_training = non_zeus.sample(frac=0.9, random_state=random_state)\n",
    "non_zeus_testing = non_zeus.drop(index=non_zeus_training.index)\n",
    "\n",
    "len(non_zeus_training), len(non_zeus_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mix up the zeus and non-zeus data sets, making sure to extract the 'Source' columns so they are excluded from modelling. Save these known classifications in a separate variable. This will be used during the training stage to map each entry to it's group, and the testing stage to determine the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set = zeus_training.append(non_zeus_training, ignore_index=True)\n",
    "training_source = training_set['Source']\n",
    "del training_set['Source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing_set = zeus_testing.append(non_zeus_testing, ignore_index=True)\n",
    "testing_source = testing_set['Source']\n",
    "del testing_set['Source']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper evaluates four methods of classifying binaries:\n",
    "  1. Support Vector Classification/Machines\n",
    "  2. Logistic Regression\n",
    "  3. Classification/Decision Trees\n",
    "  4. k-Nearest Neighbors (k-NN)\n",
    "  \n",
    "We have chosen to implement the k-NN approach described in the paper, due to it's ease of implementation using modern machine learning toolkits. In this case, we will use the `KNeighbors` classifier from [scikit-learn](http://scikit-learn.org) [1].\n",
    "\n",
    "## k-Nearest-Neighbours\n",
    "Resources:\n",
    "  - http://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/auto_examples/tutorial/plot_knn_iris.html\n",
    "  - https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn\n",
    "  - https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n",
    "  \n",
    "Resources for explanation:\n",
    "  - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4916348/\n",
    "  - https://saravananthirumuruganathan.wordpress.com/2010/05/17/a-detailed-introduction-to-k-nearest-neighbor-knn-algorithm/ (slick)\n",
    "  - [2]: E. Alpaydin. Introduction to machine learning. MIT press, 2004\n",
    "  \n",
    "  \n",
    "The book \"Introduction to machine learning\" [2]  ... TODO\n",
    "\n",
    "non-parametric estimation: only key assumption is that similar inputs hav similar outputs\n",
    "\n",
    "lazy learning algorithm\n",
    "\n",
    "how does this differ from other non-parametric estimators, is this particularly applicable to the data?\n",
    "\n",
    "Nice diagram. Can I find a GIF that shows the process?\n",
    "  - http://davpinto.com/ml-simulations/#k-nearest-neighbors-classifier\n",
    "  - \n",
    "\n",
    "Potential Advantages:\n",
    "  - There is no distinct training phase, so it can be set up to constantly learn in the field (iteratively improving our model as time goes on\n",
    "  - \n",
    "  \n",
    "  \n",
    "Potential Disadvantages:\n",
    "  - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical values\n",
    "\n",
    "The k-NN algorithm can only be used on numeric values (since it needs to take a distance metric). For each categorical column in the data set we should either remove it, or map it into a numeric space:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FileName\n",
    "These values don't seem to be the natural filename the binaries were distributed as, and are instead a concatentation of the malware type (e.g. 'zeusbin') and a hash. This column should be removed since it gives the game away slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del training_set['FileName']\n",
    "del testing_set['FileName']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TimeDateStamp\n",
    "This can easily be mapped into a UNIX epoch, which should allow computing sane distances between times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_time_date_stamp(time_date_stamp):\n",
    "    \"\"\"\n",
    "    Reads in a TimeDateStamp string and parses it, \n",
    "    returning a POSIX timestamp.\n",
    "    \n",
    "    Example TimeDateStamp:\n",
    "        0x50FDE944 [Tue Jan 22 01:20:04 2013 UTC]\n",
    "        \n",
    "    where 0x50FDE944 is a hex string representation\n",
    "    of the number of seconds from the UNIX epoch (in\n",
    "    UTC).\n",
    "    \"\"\"\n",
    "    hex_string = time_date_stamp.split()[0]\n",
    "    posix_timestamp = int(hex_string, 16)\n",
    "    return posix_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heres an example of the function being used to parse a timestamp. It then compares the parsed timestamp with the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "example = zeus.values[0][4]\n",
    "print(example)\n",
    "\n",
    "timestamp = parse_time_date_stamp(example)\n",
    "print(datetime.datetime.utcfromtimestamp(timestamp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works! Apply this conversion function to the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_set['TimeDateStamp'] = training_set['TimeDateStamp'].apply(func=parse_time_date_stamp)\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing_set['TimeDateStamp'] = testing_set['TimeDateStamp'].apply(func=parse_time_date_stamp)\n",
    "testing_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalising numeric values\n",
    "\n",
    "From the \n",
    "\n",
    "From https://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/:\n",
    "``` py\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)  \n",
    "X_test = scaler.transform(X_test)  \n",
    "```\n",
    "\n",
    "(docs: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()  \n",
    "scaler.fit(training_set)\n",
    "\n",
    "training_set = scaler.transform(training_set)\n",
    "testing_set = scaler.transform(testing_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "binary_classifier = neighbors.KNeighborsClassifier(n_neighbors=35)\n",
    "binary_classifier.fit(training_set, training_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the classifier on the whole testing set, and use the models `.score()` method for a quick sanity check of the models accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing_predictions = binary_classifier.predict(testing_set)\n",
    "binary_classifier.score(testing_set, testing_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "TODO: Neaten up, more explanation.\n",
    "\n",
    "\n",
    "The effectiveness of this model can be analysed using a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix). From the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix) of sci-kit learn:\n",
    "\n",
    ">  By definition a confusion matrix $C$ is such that $C_{i, j}$\n",
    "> is equal to the number of observations known to be in group $i$ but\n",
    "> predicted to be in group $j$.\n",
    ">\n",
    "> Thus in binary classification, the count of true negatives is\n",
    "> $C_{0,0}$, false negatives is $C_{1,0}$, true positives is\n",
    "> $C_{1,1}$ and false positives is $C_{0,1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(testing_source, testing_predictions)\n",
    "confusion_matrix = pandas.DataFrame(\n",
    "    data=confusion_matrix, \n",
    "    index=[\n",
    "        'Non-Zeus', \n",
    "        'Zeus',\n",
    "    ], \n",
    "    columns=[\n",
    "        'Predicted Non-Zeus',\n",
    "        'Predicted Zeus'\n",
    "    ],\n",
    ")\n",
    "\n",
    "confusion_figure, confusion_axes = matplotlib.pyplot.subplots()\n",
    "confusion_axes.set_title('Confusion matrix showing the predicted vs. true class \\n')\n",
    "seaborn.heatmap(\n",
    "    confusion_matrix,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=seaborn.color_palette(\"Blues\"),\n",
    "    vmin=0,\n",
    "    ax=confusion_axes,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The specificity and sensitivity of the model can be easily computed from the confusion matrix above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sensitivity(confusion_matrix):\n",
    "    true_positives = confusion_matrix['Predicted Zeus']['Zeus']\n",
    "    false_negatives = confusion_matrix['Predicted Non-Zeus']['Zeus']\n",
    "    return true_positives/(true_positives+false_negatives)\n",
    "\n",
    "print('Sensitivity: {:.2f}%'.format(\n",
    "    sensitivity(confusion_matrix)*100\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def specificity(confusion_matrix):\n",
    "    false_positives = confusion_matrix['Predicted Zeus']['Non-Zeus']\n",
    "    true_negatives = confusion_matrix['Predicted Non-Zeus']['Non-Zeus']\n",
    "    return true_negatives/(true_negatives+false_positives)\n",
    "\n",
    "print('Specificity: {:.2f}%'.format(\n",
    "    specificity(confusion_matrix)*100\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code has been able to replicate the approach of the paper, albeit with a different data set. \n",
    "\n",
    "Our model is suprisingly accurate given the limitiations of the data set, with a relatively small training set of ~700 data points. Further, the features have been generated via static analysis rather than dynamic analysis. Since dynamic analysis has also has access to te runtime behaviour the binary, the data sets generated via dynamic analysis tend to be richer than those generated statically. This can be seen in the proliferation of JIT approaches to programming language runtimes in recent times [TODO: reference someting concrete here?].\n",
    "\n",
    "Given the above, it is worth examining the performance of this model more closely. One approach could be to apply multiple rounds of cross-validation to the current data set. This should reduce the variability in our performance estimates.\n",
    "\n",
    "\n",
    "\n",
    "## k-Fold Cross-Validation\n",
    "TODO: Use K-fold validation to more accurately score our predictions? https://en.wikipedia.org/wiki/Cross-validation_(statistics)\n",
    "   * This should decrease the variance in our accuracy score:\n",
    "     > To reduce variability, in most methods multiple rounds of cross-validation are performed using different partitions, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the modelâ€™s predictive performance. \n",
    "     \n",
    "Move the analysis above into a repeatable, self-contained function. This can then be run on different subsets of the data as required when running k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_knn_model(number_of_neighbors, weights, training_set, training_source, testing_set, testing_source):\n",
    "\n",
    "    # Remove FileName as it's a give away\n",
    "    del training_set['FileName']\n",
    "    del testing_set['FileName']\n",
    "\n",
    "    # Parse TimeDateStamp into a UNX timestamp\n",
    "    training_set['TimeDateStamp'] = training_set['TimeDateStamp'].apply(func=parse_time_date_stamp)\n",
    "    testing_set['TimeDateStamp'] = testing_set['TimeDateStamp'].apply(func=parse_time_date_stamp)\n",
    "\n",
    "    # Normalise the numeric values\n",
    "    scaler = preprocessing.StandardScaler()  \n",
    "    scaler.fit(training_set)\n",
    "    \n",
    "    training_set = scaler.transform(training_set)\n",
    "    testing_set = scaler.transform(testing_set)\n",
    "\n",
    "    # Train the model\n",
    "    binary_classifier = neighbors.KNeighborsClassifier(n_neighbors=number_of_neighbors, weights=weights)\n",
    "    binary_classifier.fit(training_set, training_source)\n",
    "\n",
    "    # Run the model\n",
    "    testing_predictions = binary_classifier.predict(testing_set)\n",
    "\n",
    "    # Analysis\n",
    "    confusion_matrix = metrics.confusion_matrix(testing_source, testing_predictions)\n",
    "    confusion_matrix = pandas.DataFrame(\n",
    "        data=confusion_matrix, \n",
    "        index=[\n",
    "            'Non-Zeus', \n",
    "            'Zeus',\n",
    "        ], \n",
    "        columns=[\n",
    "            'Predicted Non-Zeus',\n",
    "            'Predicted Zeus'\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mix up the zeus and non-zeus data sets\n",
    "binaries = zeus.append(non_zeus, ignore_index=True)\n",
    "\n",
    "# Separate each binary's class/source\n",
    "sources = binaries['Source']\n",
    "del binaries['Source']\n",
    "\n",
    "k_fold_splitter = model_selection.StratifiedKFold(n_splits=10,  random_state=random_state)\n",
    "splits = k_fold_splitter.split(\n",
    "    binaries,  # data to be split\n",
    "    sources,  # target/class to split by\n",
    ")\n",
    "\n",
    "confusion_matrixes= [\n",
    "    train_and_evaluate_knn_model(\n",
    "        35,\n",
    "        'uniform',\n",
    "        binaries.iloc[train_indexes],\n",
    "        sources.iloc[train_indexes],\n",
    "        binaries.iloc[test_indexes],\n",
    "        sources.iloc[test_indexes],\n",
    "    ) \n",
    "    for train_indexes, test_indexes in splits\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum_confusion_figure, sum_confusion_axes = matplotlib.pyplot.subplots()\n",
    "sum_confusion_axes.set_title(\n",
    "    'Confusion matrix showing predicted vs. true class summed\\n'\n",
    "    'over all 10 cross-validation iterations for k-NN model\\n'\n",
    ")\n",
    "seaborn.heatmap(\n",
    "    sum(confusion_matrixes),\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=seaborn.color_palette(\"Blues\"),\n",
    "    vmin=0,\n",
    "    ax=sum_confusion_axes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "average_sensitivity = sum([sensitivity(confusion_matrix) for confusion_matrix in confusion_matrixes])/10\n",
    "print('Average Sensitivity: {:.2f}%'.format(\n",
    "    average_sensitivity*100\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "average_specificity = sum([specificity(confusion_matrix) for confusion_matrix in confusion_matrixes])/10\n",
    "print('Average Specificity: {:.2f}%'.format(\n",
    "    average_specificity*100\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By considering a number of ... TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted k-NN by distance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_fold_splitter = model_selection.StratifiedKFold(n_splits=10,  random_state=random_state)\n",
    "splits = k_fold_splitter.split(\n",
    "    binaries,  # data to be split\n",
    "    sources,  # target/class to split by\n",
    ")\n",
    "\n",
    "confusion_matrixes= [\n",
    "    train_and_evaluate_knn_model(\n",
    "        35,\n",
    "        'distance',\n",
    "        binaries.iloc[train_indexes],\n",
    "        sources.iloc[train_indexes],\n",
    "        binaries.iloc[test_indexes],\n",
    "        sources.iloc[test_indexes],\n",
    "    ) \n",
    "    for train_indexes, test_indexes in splits\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weighted_confusion_figure, weighted_confusion_axes = matplotlib.pyplot.subplots()\n",
    "weighted_confusion_axes.set_title(\n",
    "    'Confusion matrix showing predicted vs. true class summed\\n'\n",
    "    'over all 10 cross-validation iterations for weighted k-NN model\\n'\n",
    ")\n",
    "seaborn.heatmap(\n",
    "    sum(confusion_matrixes),\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=seaborn.color_palette(\"Blues\"),\n",
    "    vmin=0,\n",
    "    ax=weighted_confusion_axes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "average_sensitivity = sum([sensitivity(confusion_matrix) for confusion_matrix in confusion_matrixes])/10\n",
    "print('Average Sensitivity: {:.2f}%'.format(\n",
    "    average_sensitivity*100\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "average_specificity = sum([specificity(confusion_matrix) for confusion_matrix in confusion_matrixes])/10\n",
    "print('Average Specificity: {:.2f}%'.format(\n",
    "    average_specificity*100\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[1]: [Data Science Toolbox, Lecture 01](https://www.ole.bris.ac.uk/bbcswebdav/pid-3554286-dt-content-rid-9984867_2/xid-9984867_2). Daniel Lawson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference (time series with bro log)\n",
    "[1]:Markus Bogl, Wolfgang Aigner, Peter Filzmoser, Tim Lammarsch, Silvia Miksch, and Alexander Rind. Visual Analytics for Model Selection in Time Series Analysis, 2013 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References (k-NN)\n",
    "\n",
    "[0]: Abedelaziz Mohaisen and Omar Alrawi. 2013. Unveiling Zeus: automated classification of malware samples. In Proceedings of the 22nd International Conference on World Wide Web (WWW '13 Companion). ACM, New York, NY, USA, 829-832. DOI: https://doi.org/10.1145/2487788.2488056 PDF: https://alrawi.github.io/static/papers/unzeus_www13.pdf\n",
    "\n",
    "[1]: [Scikit-learn: Machine Learning in Python](http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html), Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n",
    "\n",
    "[2]: E. Alpaydin. Introduction to machine learning. MIT press, 2004. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
